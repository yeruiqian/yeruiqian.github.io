<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>[论文精读] StyleGAN2 论文&amp;代码理解 (上) | RuiqianYe</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://yeruiqian.github.io/favicon.ico?v=1722758233249">
<link rel="stylesheet" href="https://yeruiqian.github.io/styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="
一、前言
二、简要介绍
三、详细解析

1、归一化的修改

1.1生成器结构的修改
1.2重新审视实例归一化(Instance normalization)


2、图像质量和生成器平滑(PPL相关内容)
3、关于渐进式增长Progres..." />
    <meta name="keywords" content="原理理解" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://yeruiqian.github.io">
        <img src="https://yeruiqian.github.io/images/avatar.png?v=1722758233249" class="site-logo">
        <h1 class="site-title">RuiqianYe</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
          <a class="social-link" href="https://github.com/yeruiqian" target="_blank">
            <i class="fab fa-github"></i>
          </a>
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      温故而知新
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://yeruiqian.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">[论文精读] StyleGAN2 论文&amp;代码理解 (上)</h2>
            <div class="post-date">2024-08-03</div>
            
            <div class="post-content" v-pre>
              <p><ul class="markdownIt-TOC">
<li><a href="#%E4%B8%80-%E5%89%8D%E8%A8%80">一、前言</a></li>
<li><a href="#%E4%BA%8C-%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D">二、简要介绍</a></li>
<li><a href="#%E4%B8%89-%E8%AF%A6%E7%BB%86%E8%A7%A3%E6%9E%90">三、详细解析</a>
<ul>
<li><a href="#1-%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E4%BF%AE%E6%94%B9">1、归一化的修改</a>
<ul>
<li><a href="#11%E7%94%9F%E6%88%90%E5%99%A8%E7%BB%93%E6%9E%84%E7%9A%84%E4%BF%AE%E6%94%B9">1.1生成器结构的修改</a></li>
<li><a href="#12%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96instance-normalization">1.2重新审视实例归一化(Instance normalization)</a></li>
</ul>
</li>
<li><a href="#2-%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E5%92%8C%E7%94%9F%E6%88%90%E5%99%A8%E5%B9%B3%E6%BB%91ppl%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9">2、图像质量和生成器平滑(PPL相关内容)</a></li>
<li><a href="#3-%E5%85%B3%E4%BA%8E%E6%B8%90%E8%BF%9B%E5%BC%8F%E5%A2%9E%E9%95%BFprogressive-growing">3、关于渐进式增长Progressive growing</a>
<ul>
<li><a href="#31%E9%87%87%E7%94%A8%E6%9B%BF%E6%8D%A2%E7%BB%93%E6%9E%84alternative-network">3.1采用替换结构(Alternative network)</a></li>
<li><a href="#32%E4%B8%8D%E5%90%8C%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E4%BD%BF%E7%94%A8">3.2不同分辨率的使用</a></li>
</ul>
</li>
<li><a href="#4-%E5%9B%BE%E5%83%8F%E6%8A%95%E5%BD%B1%E5%88%B0%E9%9A%90%E7%A0%81%E7%A9%BA%E9%97%B4latent-space">4、图像投影到隐码空间(latent space)</a>
<ul>
<li><a href="#41%E6%80%8E%E4%B9%88%E6%8A%95%E5%BD%B1">4.1怎么投影？</a></li>
<li><a href="#42%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%9A%84%E5%B1%9E%E6%80%A7">4.2生成图像的属性</a></li>
</ul>
</li>
<li><a href="#%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">未来展望</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="一-前言">一、前言</h1>
<p>距离阅读这篇论文也过去几天了，有些当初的想法也有所丢失。还是得记录一下理解过程，避免这么快速的忘记。精读这篇文章的原因还是来自于一些工作中的启发，人脸修复算法(face restoration)效果较好的基于可以分为3个流派，一种基于stylegan先验的GFPGAN、GPEN等，另外两种分别是基于transform和diffusion。而基于stylegan的方式通常都是采用 stylegan2。所以有必要熟悉一下stylegan2。stylegan2 只阅读论文或只阅读代码感觉整体的理解都会比较苦涩，结合两者感觉才可以理解的比较好一些。</p>
<h1 id="二-简要介绍">二、简要介绍</h1>
<p>stylegan2 相较于 stylegan 主要改进了几个地方，根据文中作者的用语，一个很常见的词就是 <code>revisited</code> ,重新审视了一代的一些结构、损失函数、正则化、训练方式等方面，并提出改进。根据文中的章节和描述，主要分为四大块，<br>
<strong>(1)、水滴状伪影的修正(归一化的修改)，作者详细的说明一代中使用AdaIN 的缺点，然后分析更好的结构，并直接把归一化操作融合进卷积权重内，变成卷积权重归一化，十分巧妙。</strong><br>
<strong>(2)、图像质量的的提升，其实是主要提出PPL(perceptual path length)这个东西，并利用这个东西做正则化。</strong><br>
<strong>(3)、关于渐进提升分辨率的做法，分析不同分辨率下用什么结构融合会比较好(skip or residual)，不同分辨率的特征有没有充分使用上？</strong><br>
<strong>(4)、关于将图像投影到隐码 (latent space)，怎么投影？利用这个特征，图像-&gt;latent-&gt;图像 来鉴别是自然图像还是模型生成的图像</strong></p>
<h1 id="三-详细解析">三、详细解析</h1>
<h2 id="1-归一化的修改">1、归一化的修改</h2>
<p>水滴状问题如下图所示：<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722673864957.png" alt="" loading="lazy"><br>
作者将stylegan的水滴状伪影定位到其中的AdaIN归一化模块，为什么这么说？作者认为分别归一化每个特征图的均值和方差，这样子每个特征和相邻特征之间的相对大小关系就被破坏了。AdaIN可以通过让生成器制造尖峰spike来主导整体分布，从而让其他位置的信号变小，逃过鉴别器的鉴别。尖峰就形成了水滴状，一种相较于其他位置很高的信号强度。</p>
<h3 id="11生成器结构的修改">1.1生成器结构的修改</h3>
<p>作者重新审视了一下生成器的结构，做出如下图的分离与简化。<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722675366892.png" alt="" loading="lazy"><br>
图a 为原始的StyleGAN结构，图b是分离出归一化层和style block的结构，至于为啥一个AdaIN可以分解成两个mean/std层，这需要涉及到图像风格转移方面的知识。AdaIN 可以看作是两个分布的叠加，<br>
Adaptive Instance Normalization (AdaIN) 的公式如下：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>AdaIN</mtext><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo>)</mo><mo>=</mo><mi>σ</mi><mo>(</mo><mi>y</mi><mo>)</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>σ</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo fence="true">)</mo></mrow><mo>+</mo><mi>μ</mi><mo>(</mo><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\text{AdaIN}(x, y) = \sigma(y) \left( \frac{x - \mu(x)}{\sigma(x)} \right) + \mu(y) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">AdaIN</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">μ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">μ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p>
<p>其中：</p>
<ul>
<li>( x ) 是内容图像的特征映射。</li>
<li>( y ) 是风格图像的特征映射。</li>
<li>( \mu(x) ) 和 ( \sigma(x) ) 分别是内容图像特征映射的均值和标准差。</li>
<li>( \mu(y) ) 和 ( \sigma(y) ) 分别是风格图像特征映射的均值和标准差。</li>
</ul>
<p>在图b中，下面的Mod就是style嵌入过程，上面的归一化Norm就是内容的归一化x。原始 StyleGAN 在Style Block中应用偏差和噪声，导致它们的相对影响与当前在Style的大小成反比。这个可以理解Style和(偏置和噪声)同样作用于整个block，这个作用大另外一个就作用小了。作者觉得这样不太好，通过将偏置和噪声挪到Block之外，也就是针对归一化后的数据再增加偏置和噪声，可以获得更容易观测的结果。然后发现通过这个变化之后，发现仅仅对标准差Std进行归一化(Norm)和调制(Mod)就够了，不需要均值了，这个变化可以通过图c看到。这是一切重新设计的起点。</p>
<h3 id="12重新审视实例归一化instance-normalization">1.2重新审视实例归一化(Instance normalization)</h3>
<p>StyleGAN 的主要优点之一是能够通过风格混合来控制生成的图像，即在推理时将不同的latent w 馈送到不同的层。这种方式可能会放大某些特征图的特征，因此这就是后面归一化操作Norm的存在意义，抵消这种放大，这样后续层可以继续控制图像的生成方向。因此去掉归一化操作的话虽然可以解决水滴伪影问题，却会失去一些尺度的控制。<br>
因此，作者提出一种替代的归一化方案，这种方案可以两者兼顾(既要还要)，同时解决水滴伪影问题并能够保持整体特征可控制。主要的思想是对要传入的特征图的数据进行归一化，但并不是强制的归一化！<br>
我们看下图2c，每一个Style Block包含3个东西，调制(Mod)、卷积、归一化(Norm)，调制的话就是将一些特征图的作用放大，作者想到，哎这是可以通过直接修改卷积权重做到吗？weight x scale = feature map x scale，于是，前面两个操作可以用一个卷积替换！<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722677250688.png" alt="" loading="lazy"><br>
到这里或许会想，那后面那个归一化(Norm) 能不能也简化掉？<br>
当然可以！后面的归一化操作就是为了抵消放大倍数s的影响，那只要计算他把方差放大了s，我们再用乘以 (1/s) 不就可以了？<br>
以输入单位标准差为例，经过Mod，标准差变成(式子2)，这是一个L2范数的结果，因此，解调(demod)也就是后续的Norm操作需要将标准差重新变成单位标准差，也就是需要除以sigma，那么这个操作又可以直接用卷积权重来实现(式子3)。因此整体调整后的结构就是图2d。这个调制方式并不是直接使用特征图，而是基于统计数据。<br>
(ps：为了使实现更高效，作者使用了分组卷积，为了让式子3成立，作用为激活函数进行了缩放，这些都在代码中体现)</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722677774029.png" alt="图片2" width="400">
    <img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722677791203.png" alt="图片1" width="400">
</div>
<h2 id="2-图像质量和生成器平滑ppl相关内容">2、图像质量和生成器平滑(PPL相关内容)</h2>
<p>文章指出虽然FID和P&amp;R指标在一定程度上可以作为生成器的指标，但是有些场景却不能够覆盖，并举了图13的例子，相同参数但是质量却差别很大。作者观察到感知图像质量和感知路径长度 (PPL) 之间的相关性，通过测量在隐码空间(latent space)发生小扰动情况下，生成图像之间的平均 LPIPS 距离，来量化从隐码空间到输出图像的映射平滑度。也就是给latent space添加点噪声，看生成图像之间的差距。小的PPL距离代表着较高的图像质量。作者通过图4举了个例子，通过在latent space采样，低PPL的图像质量高于高PPL的。<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722697259428.png" alt="图4" loading="lazy"><br>
作者猜想PPL和图像质量的关系，认为在训练的时候由于鉴别器的惩罚作用，生成器最好的选择就是将高质量图像的latent space 拉大(让空间更平滑)，让质量较差图像的latent space变得又急又小，这样鉴别器的损失就可以达到很小。但是作者提出，虽然低PPL代表图像，但并不是要等于0，因为等于0，生成图像就很单一，召回率就等于0了，训练图像不能被完全复现。因此，作者提出了一种基于PPL的正则化器，来制造更加平滑的生成器映射。由于正则化计算是很耗费资源的，因此作者首先提出一种适用于正则化的新优化点。也就是Lazy regularization 懒惰正则化。<br>
什么是Lazy regularization，顾名思义就是让正则化不那么频繁的更新，这样会减少整体训练的时间，对资源的损耗会降低，论文中提到生成器的主要损失函数是logistics loss 和 正则化项(regularization terms)，但作者通过实验证明，其实正则化项并不需要每次都更新，他每隔16次迭代更新一次就行，并不会影响生成器的结果。详情见表1 c行<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722698058582.png" alt="表1" loading="lazy"><br>
新的正则化优化技术同样可以用于作者新提出的基于路径长度正则化(path length regularization)。啥是路径长度正则化？ 也就是通过在图像空间引入随机方向（随机噪声），计算latent w的梯度，理想情况下，无论方向怎么变化，w的梯度都要接近相等的长度，这就表明latent spcae和image space映射良好，也就是说，如果w中发生了固定大小a的变化，那么图像也会发生中固定大小b的变化。<br>
正则化器可以设置为下式。<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722698942479.png" alt="" loading="lazy"><br>
其中Jw是Jacobian matrix，也即生成器G(w) 和w的偏导数 （也就是图像y和w的偏导数），从式子中看，当偏导数和图像y正交，这个距离是最小的，损失也是最小的。由于考虑到Jacobian matrix矩阵的计算成本，作者直接用梯度*y代替，a也随着时间变化，是一个移动均值的结果，如下图所述。<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722699433427.png" alt="替代正则化" loading="lazy"><br>
从图5中可以看到，采用这种正则化技术可以让PPL变得更加紧凑(右侧)，整体分布都比较小(代表高质量)，但又不是0（召回率为0）<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722699503644.png" alt="" loading="lazy"></p>
<h2 id="3-关于渐进式增长progressive-growing">3、关于渐进式增长Progressive growing</h2>
<p>作者指出渐进式增长在生成高分辨率图像方面很成功，但同时也存在问题，有很强的位置偏好。例如图6所示，在转头过程中原来应该一起移动的牙齿却固定不变。<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722699798101.png" alt="图6" loading="lazy"><br>
作者认为问题在于在渐进式增长中，每个分辨率暂时用作输出分辨率，迫使它生成最高频细节，这导致经过训练的网络在中间层具有过高的频率，从而出现移位不变性。作者想要在去掉缺点的前提下，保留渐进式增长的好处。主要做了两个事情。(1)采用其他结构，skip or residual network。(2) 观察不同分辨率对图像的贡献。</p>
<h3 id="31采用替换结构alternative-network">3.1采用替换结构(Alternative network)</h3>
<p>通过简化MSG-GAN的不同分辨率的连接方式，来找到更好的结构，并通过实验结果确定了生成器应该用跳跃(skip)结构，鉴别器应该用残差结构(residual net)。具体结构和结果见下图。</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722736979572.png" alt="图片2" width="400">
    <img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722736997376.png" alt="图片1" width="400">
</div>
<h3 id="32不同分辨率的使用">3.2不同分辨率的使用</h3>
<p>采用上图7这种结构，并不是强制性的将低分辨率的图像特征转移到高分辨率图像，而是由网络自己优化确定方向，因此作者想要通过数据量化的方式来找到生成器在训练过程中依赖于特定分辨率的程度。具体怎么做呢？作者考虑到由于跳过生成器(图7b)通过显式地对来自多个分辨率的RGB值求和来生成图像，就可以通过测量它们对最终图像的贡献程度来估计相应层的相对重要性。在图 8a 中，作者将每个 tRGB 层产生的像素值的标准差绘制为训练时间的函数。总的计算 w 的 1024 个随机样本的标准偏差，然后并对值进行归一化，使它们总和为 100%。如图8a所示。<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722737387563.png" alt="分辨率贡献" loading="lazy"><br>
然后作者就发现，哎不对劲，怎么训练到最后，1024这个尺度的贡献还没512尺度的大？然后看了一下实际生成图像，发现1024的图像确实并没有这个尺度应该有的细节，反而像是512这种尺度加上一点锐化的图像。<br>
作者考虑到，会不会是网络不够大的原因，限制了1024的尺度贡献呢？因此作者将最大尺度的特征图数量扩大两倍，重新训练，发现确实结果正常了许多，如图8b所示，1024的贡献变大了，然后计算了一下指标，确实提升了，表1F配置所示</p>
<h2 id="4-图像投影到隐码空间latent-space">4、图像投影到隐码空间(latent space)</h2>
<h3 id="41怎么投影">4.1怎么投影？</h3>
<p>反转生成网络 g 是一个具有许多应用的有趣问题，即通过隐空间中操纵图像，但这首先需要找到图像中原本的隐码 w。这个具体是怎么做的呢？给定一个图像，我们的目的是找到这个图像对应的隐码W以及噪声N。作者是怎么做的呢？<br>
首先给定随机10000组的latent code z ， 并通过映射网络(mapping network)映射到w空间，然后计算这10000组w的平均值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mu(w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">μ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span>，并通过标准差来计算每一组w到中心<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span></span></span></span>的距离。如下列表述一样<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722738862686.png" alt="" loading="lazy"><br>
初始化设置w=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mu(w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">μ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span> ，噪声n 是单位噪声N(0,I)，w和n为训练优化的参数，学习率设置：前50次用斜坡函数从0到最大值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>λ</mi><mtext>max</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\lambda_{\text{max}}=0.1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span></span></span></span>，然后最后250迭代，用余弦时间表(cosine schedule)下降到0。前面750次迭代在计算损失函数的时候对w添加高斯噪声，强度从1到0，作者说这增加了优化的随机性并稳定全局最优。<br>
作者提到，由于现在是在优化噪声图和w，需要将信号完全摒除掉，因此在损失函数，除了对图像质量进行监督，还需要对噪声进行正则化。以下是图像质量loss，就是原始图像和在给定w、n下生成图像的LPIPS距离<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mtext>image </mtext></msub><mo>=</mo><msub><mi>D</mi><mtext>LPIPS </mtext></msub><mrow><mo fence="true">[</mo><mi>x</mi><mo separator="true">,</mo><mi>g</mi><mrow><mo fence="true">(</mo><mover accent="true"><mi mathvariant="bold">w</mi><mo>~</mo></mover><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">n</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">n</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">L_{\text {image }}=D_{\text {LPIPS }}\left[x, g\left(\tilde{\mathbf{w}}, \boldsymbol{n}_0, \boldsymbol{n}_1, \ldots\right)\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.317502em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">image </span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">LPIPS </span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6812999999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span></span></span><span style="top:-3.36344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">~</span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">n</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">n</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;">]</span></span></span></span></span>，下图是噪声正则化的具体操作，多个尺度下偏移x，y一个像素，计算点积，如果噪声是高斯分布(不带任何信号)，那么，这个损失函数就为0。注：作者说下采样每个步骤都乘以2来保持单位方差，但是在代码中好像没看到。<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722741636073.png" alt="" loading="lazy"><br>
图18就是存在噪声正则化的效果，噪声图不带有信号分量。<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722741770402.png" alt="图18" loading="lazy"></p>
<h3 id="42生成图像的属性">4.2生成图像的属性</h3>
<p>对应文中5.1章节，这个章节的目的就是提出另外一种鉴别生成图像和自然图像的方式，那就是将图像投影到隐码空间w，如果能够投影回去，那么他就是假图像(生成图像)。那么怎么看投影是不是准确呢？就是计算原来图像和重新合成图像是LPIPS距离。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mtext>LPIPS </mtext></msub><mrow><mo fence="true">[</mo><mi>x</mi><mo separator="true">,</mo><mi>g</mi><mrow><mo fence="true">(</mo><msup><mover accent="true"><mi>g</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>(</mo><mi>x</mi><mo>)</mo><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">D_{\text {LPIPS }}\left[x, g\left(\tilde{g}^{-1}(x)\right)\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">LPIPS </span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">~</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">]</span></span></span></span></span></span> 。从图10可以看到，使用StyleGAN2生成的图像可以投影到W中，并反生成十分接近的图像，就可以确定这是生成的假图像，真实图像是没有办法投影到非常接近的地步的，重新生成的图像和原图也有较大的差异。stylegan 第一代使用这种方法没办法做到，区分不开。这种方式作者说是<code>attribute a generated image to its source</code> 归源法？<br>
<img src="https://raw.githubusercontent.com/yeruiqian/blog_img/master/paper/QQ_1722742224171.png" alt="图10" loading="lazy"></p>
<h2 id="未来展望">未来展望</h2>
<p>作者的未来展望：<br>
1、研究路径长度正则化的进一步改进，例如，通过用数据驱动的特征空间指标来替换像素空间 L2 距离。<br>
2、考虑到 GAN 的实际部署，作者认为找到减少训练数据需求的新方法将很重要。因为获取大量数据在很多应用是比较难的。</p>
<p>具体代码理解放到下一篇了。。</p>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://yeruiqian.github.io/tag/4cjlwFeuR/" class="tag">
                    原理理解
                  </a>
                
              </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://yeruiqian.github.io/post/yuan-li-ke-bian-xing-juan-ji-deformable-convolutionyuan-li-ji-dai-ma-jie-shi/">
                  <h3 class="post-title">
                    [原理] 可变性卷积(deformable convolution)原理及代码解释
                  </h3>
                </a>
              </div>
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>





  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: 'Ov23liaRWNmhKYXwLF57',
        clientSecret: 'b5f0905139787275a2ea74626cf44f004c753358',
        repo: 'yeruiqian.github.io',
        owner: 'yeruiqian',
        admin: ['yeruiqian'],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
